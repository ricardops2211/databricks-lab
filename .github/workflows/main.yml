name: Databricks Login & Test (Linux Self-Hosted)

on:
  push:
    branches:
      - main

jobs:
  login_databricks_hvault:
    runs-on: self-hosted

    steps:
      # ðŸ”¹ Paso 0: Checkout del repo
      - name: Checkout
        uses: actions/checkout@v4

      # ðŸ”¹ Paso 1: Login en Vault
      - name: Login en Vault
        shell: bash
        env:
          VAULT_ADDR: http://127.0.0.1:8200
          VAULT_TOKEN: ${{ secrets.VAULT_TOKEN }}
          VAULT_SKIP_VERIFY: "true"
          PATH: ${{ env.PATH }}:/home/gha-runner/vault
        run: |
          vault --version
          vault login $VAULT_TOKEN
          vault token lookup

      # ðŸ”¹ Paso 2: Obtener token de Databricks desde Vault
      - name: Obtener token de Databricks
        id: get-creds
        shell: bash
        env:
          VAULT_ADDR: http://127.0.0.1:8200
          VAULT_SKIP_VERIFY: "true"
          PATH: ${{ env.PATH }}:/home/gha-runner/vault
        run: |
          dbToken=$(vault kv get -field=DATABRICKS_TOKEN secret/azure)
          echo "DATABRICKS_TOKEN=$dbToken" >> $GITHUB_OUTPUT

      # ðŸ”¹ Paso 3: Listar clusters con Databricks CLI
      - name: Listar clusters de Databricks con CLI
        shell: bash
        env:
          DATABRICKS_HOST: "https://adb-3120226433905944.4.azuredatabricks.net"
          DATABRICKS_TOKEN: ${{ steps.get-creds.outputs.DATABRICKS_TOKEN }}
          PATH: ${{ env.PATH }}:~/databricks-env/bin
        run: |
          source ~/databricks-env/bin/activate
          echo "Lista de clusters:"
          databricks clusters list

      # ðŸ”¹ Paso 4: Crear clusters desde JSON (CLI antigua)
      - name: Crear clusters Databricks desde JSON
        shell: bash
        env:
          DATABRICKS_HOST: "https://adb-3120226433905944.4.azuredatabricks.net"
          DATABRICKS_TOKEN: ${{ steps.get-creds.outputs.DATABRICKS_TOKEN }}
          PATH: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:~/databricks-env/bin:~/vault
        run: |
          source ~/databricks-env/bin/activate
          mkdir -p clusters_created
          
          for cluster_file in clusters/*.json; do
            echo "Creando cluster desde $cluster_file..."
            # Crear cluster y extraer cluster_id usando grep
            CLUSTER_JSON=$(databricks clusters create --json "$(cat $cluster_file)")
            CLUSTER_ID=$(echo "$CLUSTER_JSON" | grep -oP '"cluster_id": "\K[^"]+')
            
            echo "$CLUSTER_ID" > clusters_created/$(basename $cluster_file .json).id
            
            echo "Esperando cluster $CLUSTER_ID..."
            until [ "$(databricks clusters get --cluster-id $CLUSTER_ID | grep -oP '"state": "\K[^"]+')" == "RUNNING" ]; do
              sleep 15
            done
            echo "Cluster $CLUSTER_ID listo!"
          done

      # ðŸ”¹ Paso 5: Importar y ejecutar notebooks
      - name: Importar y ejecutar notebooks
        shell: bash
        env:
          DATABRICKS_HOST: "https://adb-3120226433905944.4.azuredatabricks.net"
          DATABRICKS_TOKEN: ${{ steps.get-creds.outputs.DATABRICKS_TOKEN }}
          PATH: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:~/databricks-env/bin:~/vault
        run: |
          source ~/databricks-env/bin/activate
          
          for notebook in notebooks/*.py; do
            NOTEBOOK_NAME=$(basename $notebook)
            echo "Importando notebook $NOTEBOOK_NAME..."
            databricks workspace import "$notebook" "/Users/gha-runner/$NOTEBOOK_NAME" --language PYTHON --overwrite

            for cluster_id_file in clusters_created/*.id; do
              CLUSTER_ID=$(cat $cluster_id_file)
              echo "Ejecutando $NOTEBOOK_NAME en cluster $CLUSTER_ID..."
              databricks runs submit --json "{
                \"run_name\": \"Run $NOTEBOOK_NAME desde GHA\",
                \"existing_cluster_id\": \"$CLUSTER_ID\",
                \"notebook_task\": {\"notebook_path\": \"/Users/gha-runner/$NOTEBOOK_NAME\"}
              }"
            done
          done
