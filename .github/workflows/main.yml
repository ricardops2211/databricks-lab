name: Databricks-Job

on:
  push:
    branches:
      - main

jobs:
  databricks-ejecucion:
    runs-on: self-hosted
    env:
      DATABRICKS_HOST: "https://adb-2011990214744919.19.azuredatabricks.net"

    steps:
      # üîπ Paso 0: Checkout del repo
      - name: Checkout
        uses: actions/checkout@v4

      - name: Login en Vault
        shell: bash
        env:
          VAULT_ADDR: http://127.0.0.1:8200
          VAULT_TOKEN: ${{ secrets.VAULT_TOKEN }}
          VAULT_SKIP_VERIFY: "true"
          PATH: ${{ env.PATH }}:/home/gha-runner/vault
        run: |
          set -Eeuo pipefail
          vault --version
          vault login "$VAULT_TOKEN" >/dev/null
          vault token lookup

      - name: Obtener token de Databricks
        id: get-creds
        shell: bash
        env:
          VAULT_ADDR: http://127.0.0.1:8200
          VAULT_SKIP_VERIFY: "true"
          PATH: ${{ env.PATH }}:/home/gha-runner/vault
        run: |
          set -Eeuo pipefail
          dbToken=$(vault kv get -field=DATABRICKS_TOKEN secret/azure)
          echo "::add-mask::$dbToken"
          # ‚úîÔ∏è exporta a entorno para las siguientes steps
          echo "DATABRICKS_TOKEN=$dbToken" >> "$GITHUB_ENV"
          # (Si igual quieres como output, puedes mantener:)
          echo "DATABRICKS_TOKEN=$dbToken" >> "$GITHUB_OUTPUT"

      
      # üîπ Paso 3: Importar notebooks en /Shared/notebook/py
      - name: Importar notebooks en Databricks
        shell: bash
        env:
          PATH: ${{ env.PATH }}:~/databricks-env/bin
        run: |
          source ~/databricks-env/bin/activate

          databricks workspace mkdirs /Shared/notebook/py

          for notebook in notebooks/*.py; do
            NOTEBOOK_NAME="${notebook##*/}"
            echo "Importando $NOTEBOOK_NAME en /Shared/notebook/py..."
            databricks workspace import "$notebook" "/Shared/notebook/py/$NOTEBOOK_NAME" \
              --language PYTHON --overwrite
          done

      # üîπ Paso 4: Subir archivos de data y configuraci√≥n a DBFS
      - name: Subir archivos de data y config a DBFS
        shell: bash
        env:
          PATH: ${{ env.PATH }}:~/databricks-env/bin
        run: |
          source ~/databricks-env/bin/activate

          # Crear carpeta de data y raw
          databricks fs mkdirs dbfs:/FileStore/data
          databricks fs mkdirs dbfs:/FileStore/jobs_data/raw

          # Subir archivos/carpetas de data
          for file in data/*; do
            FILENAME="${file##*/}"
            if [ -d "$file" ]; then
              echo "üì§ Subiendo carpeta $FILENAME a DBFS..."
              databricks fs cp -r "$file" dbfs:/FileStore/data/$FILENAME --overwrite
            else
              echo "üì§ Subiendo archivo $FILENAME a DBFS..."
              databricks fs cp "$file" dbfs:/FileStore/data/$FILENAME --overwrite
            fi
          done

          # Subir archivos espec√≠ficos a jobs_data/raw
          databricks fs cp data/raw/clientes.csv dbfs:/FileStore/jobs_data/raw/clientes.csv --overwrite
          databricks fs cp data/raw/ventas.json dbfs:/FileStore/jobs_data/raw/ventas.json --overwrite

          # Crear carpeta de configuraci√≥n
          databricks fs mkdirs dbfs:/FileStore/jobs_data/config

          # Subir parametros.yaml
          databricks fs cp data/config/parametros.yaml dbfs:/FileStore/jobs_data/config/parametros.yaml --overwrite

          # Verificar archivos subidos
          echo "üìÇ Listado de dbfs:/FileStore/data"
          databricks fs ls dbfs:/FileStore/data

          echo "üìÇ Listado de dbfs:/FileStore/jobs_data/raw"
          databricks fs ls dbfs:/FileStore/jobs_data/raw

          echo "üìÇ Listado de dbfs:/FileStore/jobs_data/config"
          databricks fs ls dbfs:/FileStore/jobs_data/config


      # üîπ Paso 5: Verificar archivos subidos
      - name: Verificar archivos subidos a DBFS
        shell: bash
        env:
          PATH: ${{ env.PATH }}:~/databricks-env/bin
        run: |
          source ~/databricks-env/bin/activate

          echo "üìÇ Listando archivos en dbfs:/FileStore/data"
          databricks fs ls dbfs:/FileStore/data

          echo "üìÇ Listando archivos en dbfs:/FileStore/jobs_data/raw"
          databricks fs ls dbfs:/FileStore/jobs_data/raw

          echo "üìÇ Listando archivos en dbfs:/FileStore/jobs_data/config"
          databricks fs ls dbfs:/FileStore/jobs_data/config

          # Probar si parametros.yaml existe
          PARAM_FILE="dbfs:/FileStore/jobs_data/config/parametros.yaml"
          if databricks fs ls "$PARAM_FILE" >/dev/null 2>&1; then
            echo "‚úÖ parametros.yaml existe en DBFS"
          else
            echo "‚ùå parametros.yaml NO existe en DBFS"
            exit 1
          fi

      # üîπ 7) UPSERT de Jobs (script Python)
      - name: Crear/Actualizar Job(s) en Databricks (UPSERT)
        shell: bash
        env:
          JOBS_DIR: jobs
          JOB_IDS_PATH: .gha/job_ids.json
        run: |
          set -Eeuo pipefail
          python3 scripts/create-jobdatabricks.py

      # üîπ 8) Ejecutar Job(s) y monitorear (script Python)
      - name: Ejecutar Job(s) y verificar
        shell: bash
        env:
          JOB_IDS_PATH: .gha/job_ids.json      # o define JOB_ID si vas a uno puntual
          POLL_SECONDS: "10"
          TAIL_INTERVAL: "10"
          TIMEOUT_SECONDS: "7200"
          # Si tu destino no es el default, fija el base:
          LOGS_DBFS_BASE: "dbfs:/cluster-logs"
        run: |
          set -Eeuo pipefail
          python3 scripts/execute-jobdatabricks.py



      # üîπ Paso 7: Descargar outputs desde DBFS
      - name: Descargar outputs de DBFS
        shell: bash
        env:
          PATH: ${{ env.PATH }}:~/databricks-env/bin
        run: |
          source ~/databricks-env/bin/activate

          OUTPUT_DIR=outputs/
          DBFS_DIR=dbfs:/FileStore/jobs_output/

          # Crear directorio local con Python
          python3 -c "import os; os.makedirs('$OUTPUT_DIR', exist_ok=True)"

          echo "üì• Descargando resultados de $DBFS_DIR a $OUTPUT_DIR"
          databricks fs cp -r "$DBFS_DIR" "$OUTPUT_DIR"
          echo "‚úÖ Outputs listos en $OUTPUT_DIR"

          # Listar archivos usando Python
          echo "üìÑ Listado de archivos en $OUTPUT_DIR:"
          python3 -c "import os; [print(os.path.join(dp, f)) for dp, dn, filenames in os.walk('$OUTPUT_DIR') for f in filenames]"

      - name: Subir outputs como artifacts
        uses: actions/upload-artifact@v4
        with:
          name: job-outputs
          path: outputs/
          retention-days: 7

