name: Databricks Login & Test (Linux Self-Hosted)

on:
  push:
    branches:
      - main

jobs:
  login_databricks_hvault:
    runs-on: self-hosted

    steps:
      # ðŸ”¹ Paso 0: Checkout del repo
      - name: Checkout
        uses: actions/checkout@v4

      # ðŸ”¹ Paso 1: Login en Vault
      - name: Login en Vault
        shell: bash
        env:
          VAULT_ADDR: http://127.0.0.1:8200
          VAULT_TOKEN: ${{ secrets.VAULT_TOKEN }}
          VAULT_SKIP_VERIFY: "true"
          PATH: ${{ env.PATH }}:/home/gha-runner/vault
        run: |
          vault --version
          vault login $VAULT_TOKEN
          vault token lookup

      # ðŸ”¹ Paso 2: Obtener token de Databricks desde Vault
      - name: Obtener token de Databricks
        id: get-creds
        shell: bash
        env:
          VAULT_ADDR: http://127.0.0.1:8200
          VAULT_SKIP_VERIFY: "true"
          PATH: ${{ env.PATH }}:/home/gha-runner/vault
        run: |
          dbToken=$(vault kv get -field=DATABRICKS_TOKEN secret/azure)
          echo "DATABRICKS_TOKEN=$dbToken" >> $GITHUB_OUTPUT

      
      # ðŸ”¹ Paso 3: Importar notebooks en /Shared/notebook/py
      - name: Importar notebooks en Databricks
        shell: bash
        env:
          DATABRICKS_HOST: "https://adb-34259498345146.6.azuredatabricks.net"
          DATABRICKS_TOKEN: ${{ steps.get-creds.outputs.DATABRICKS_TOKEN }}
          PATH: ${{ env.PATH }}:~/databricks-env/bin
        run: |
          source ~/databricks-env/bin/activate

          databricks workspace mkdirs /Shared/notebook/py

          for notebook in notebooks/*.py; do
            NOTEBOOK_NAME="${notebook##*/}"
            echo "Importando $NOTEBOOK_NAME en /Shared/notebook/py..."
            databricks workspace import "$notebook" "/Shared/notebook/py/$NOTEBOOK_NAME" \
              --language PYTHON --overwrite
          done

      # ðŸ”¹ Paso 4: Subir archivos de data a DBFS
      - name: Subir archivos de data a DBFS
        shell: bash
        env:
          DATABRICKS_HOST: "https://adb-34259498345146.6.azuredatabricks.net"
          DATABRICKS_TOKEN: ${{ steps.get-creds.outputs.DATABRICKS_TOKEN }}
          PATH: ${{ env.PATH }}:~/databricks-env/bin
        run: |
          source ~/databricks-env/bin/activate

          databricks fs mkdirs dbfs:/FileStore/data

          for file in data/*; do
            FILENAME="${file##*/}"
            if [ -d "$file" ]; then
              echo "ðŸ“¤ Subiendo carpeta $FILENAME a DBFS..."
              databricks fs cp -r "$file" dbfs:/FileStore/data/$FILENAME --overwrite
            else
              echo "ðŸ“¤ Subiendo archivo $FILENAME a DBFS..."
              databricks fs cp "$file" dbfs:/FileStore/data/$FILENAME --overwrite
            fi
          done

      # ðŸ”¹ Paso 5: Crear Job Workflow en Databricks
      - name: Crear Job Workflow en Databricks
        shell: bash
        env:
          DATABRICKS_HOST: "https://adb-34259498345146.6.azuredatabricks.net"
          DATABRICKS_TOKEN: ${{ steps.get-creds.outputs.DATABRICKS_TOKEN }}
          PATH: ${{ env.PATH }}:~/databricks-env/bin
        run: |
          source ~/databricks-env/bin/activate

          for job_file in jobs/*.json; do
            echo "ðŸš€ Creando job workflow desde $job_file..."
            databricks jobs create --version=2.1 --json "$( < "$job_file" )"
          done

      - name: Ejecutar Job Workflow y verificar
        shell: bash
        env:
          DATABRICKS_HOST: "https://adb-34259498345146.6.azuredatabricks.net"
          DATABRICKS_TOKEN: ${{ steps.get-creds.outputs.DATABRICKS_TOKEN }}
          PATH: ${{ env.PATH }}:~/databricks-env/bin
        run: |
          source ~/databricks-env/bin/activate

          # FunciÃ³n portable para sleep
          wait_seconds() {
            python3 -c "import time; time.sleep($1)"
          }

          for job_file in jobs/*.json; do
            JOB_NAME=$(python3 -c "import json;print(json.load(open('$job_file'))['name'])")
            echo "Ejecutando job workflow $JOB_NAME..."

            JOB_JSON=$(databricks jobs create --json "$( < "$job_file" )")
            JOB_ID=$(python3 -c "import sys,json;print(json.load(sys.stdin)['job_id'])" <<< "$JOB_JSON")

            RUN_JSON=$(databricks jobs run-now --job-id "$JOB_ID")
            RUN_ID=$(python3 -c "import sys,json;print(json.load(sys.stdin)['run_id'])" <<< "$RUN_JSON")

            echo "Esperando a que termine el job..."
            while true; do
              RUN_INFO=$(databricks runs get --run-id "$RUN_ID")
              STATE=$(python3 -c "import sys,json;print(json.load(sys.stdin)['state']['life_cycle_state'])" <<< "$RUN_INFO")
              RESULT=$(python3 -c "import sys,json;print(json.load(sys.stdin)['state'].get('result_state'))" <<< "$RUN_INFO")

              echo "Estado: $STATE, Resultado: $RESULT"

              if [[ "$STATE" == "TERMINATED" ]]; then
                if [[ "$RESULT" == "SUCCESS" ]]; then
                  echo "âœ… Job $JOB_NAME terminado correctamente"
                  break
                else
                  echo "âŒ Job $JOB_NAME fallÃ³ con resultado: $RESULT"
                  databricks runs get-output --run-id "$RUN_ID" | python3 -m json.tool
                  exit 1
                fi
              fi

              # Reemplazo de sleep
              wait_seconds 15
            done
          done


      # ðŸ”¹ Paso 7: Descargar outputs desde DBFS
      - name: Descargar outputs de DBFS
        shell: bash
        env:
          DATABRICKS_HOST: "https://adb-34259498345146.6.azuredatabricks.net"
          DATABRICKS_TOKEN: ${{ steps.get-creds.outputs.DATABRICKS_TOKEN }}
          PATH: ${{ env.PATH }}:~/databricks-env/bin
        run: |
          source ~/databricks-env/bin/activate
          OUTPUT_DIR=outputs/
          DBFS_DIR=dbfs:/FileStore/jobs_output/
          mkdir -p "$OUTPUT_DIR"

          echo "ðŸ“¥ Descargando resultados de $DBFS_DIR a $OUTPUT_DIR"
          databricks fs cp -r "$DBFS_DIR" "$OUTPUT_DIR"
          echo "âœ… Outputs listos en $OUTPUT_DIR"
          ls -R "$OUTPUT_DIR"
