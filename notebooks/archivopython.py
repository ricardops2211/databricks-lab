# Databricks notebook source
# COMMAND ----------
dbutils.widgets.text("input_path", "dbfs:/mnt/input/data.csv", "Input Path")
dbutils.widgets.text("output_path", "dbfs:/mnt/output/results/", "Output Path")

input_path = dbutils.widgets.get("input_path")
output_path = dbutils.widgets.get("output_path")

print(f"ğŸ“¥ Leyendo datos desde: {input_path}")
print(f"ğŸ’¾ Guardando resultados en: {output_path}")

# Ejemplo: leer CSV, hacer transformaciÃ³n y guardar
df = spark.read.option("header", "true").csv(input_path)

# PequeÃ±a transformaciÃ³n: contar registros por columna "category"
result = df.groupBy("category").count()

result.write.mode("overwrite").parquet(output_path)

print("âœ… Proceso ETL finalizado correctamente")
